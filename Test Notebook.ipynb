{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from google_drive_api import select_files_from_google_drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = 'fx_XBTUSD+KRKN+Curncy_20180424.csv'\n",
    "val_type  = {'BID':0, 'ASK':1, 'TRADE':2}\n",
    "\n",
    "#Data parameters\n",
    "input_length     = 12\n",
    "input_dimension  = 3\n",
    "output_dimension = 3\n",
    "\n",
    "#Training parameters\n",
    "rnn_dim    = 256\n",
    "epochs     = 100\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'googleapiclient'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-52074ad6aea2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# From Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle_drive_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mselect_files_from_google_drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mcross\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'XBTUSD'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mexchange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'BGN'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cvml/Dev/fx/google_drive_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogleapiclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscovery\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhttplib2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHttp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moauth2client\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'googleapiclient'"
     ]
    }
   ],
   "source": [
    "### Load the data\n",
    "\n",
    "# Example 1\n",
    "data   = [[[0.9,0.99,0.98],[0.99,1,1]]]\n",
    "labels = [[1,1,1]]\n",
    "\n",
    "# From file .CSV\n",
    "df = pd.read_csv(input_csv, index_col=0, parse_dates=[2])\n",
    "\n",
    "# From Google Drive \n",
    "from google_drive_api import select_files_from_google_drive\n",
    "cross = 'XBTUSD'\n",
    "exchange = 'BGN'\n",
    "start_date = pd.to_datetime('2000-01-01')\n",
    "end_date = pd.to_datetime('today')\n",
    "start_date = pd.to_datetime('2018-09-25')\n",
    "df = select_files_from_google_drive(cross, exchange, start_date, end_date)\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.pivot_table(values='value', index='time', columns='type', aggfunc='sum')\n",
    "df /= 9000\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def return_raw_data(df, input_length):\n",
    "    doc = {}\n",
    "    #for idx, r in df.iterrows():\n",
    "    #    dt = idx\n",
    "    #    df_x = df[(df.index-dt)<pd.to_timedelta(1, unit='H')]\n",
    "    #    df_x = df_x[(df_x.index-dt)>=pd.to_timedelta(0, unit='H')]\n",
    "    #    df_y = df[df.index>(dt+pd.to_timedelta(2, unit='H'))]\n",
    "    #    df_y = df_y[df_y.index==df_y.index.min()]\n",
    "    #    doc[dt] = {'x': df_x, 'y':df_y}\n",
    "    for i in range(df.shape[0]-input_length):\n",
    "        dt = df.index[i]\n",
    "        df_x = df.iloc[i:i+input_length]\n",
    "        df_y = df[df.index>(dt+pd.to_timedelta(2, unit='H'))]\n",
    "        df_y = df_y[df_y.index==df_y.index.min()]\n",
    "        if not df_y.empty:\n",
    "            doc[dt] = {'x': df_x, 'y':df_y}\n",
    "    return doc\n",
    "\n",
    "doc = return_raw_data(df, input_length)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_col = ['ASK', 'BID', 'TRADE']\n",
    "X_raw = [np.array(doc[elem]['x'][filter_col]).tolist() for elem in doc]\n",
    "Y_raw = [np.array(doc[elem]['y'][filter_col]).tolist() for elem in doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalize the data\n",
    "# (later)\n",
    "\n",
    "# TODO: Divide the data in training (first 80%) and testing (last 20%)\n",
    "split_idx = int(len(X_raw)*0.8)\n",
    "X_train = X_raw[:split_idx]\n",
    "X_test = X_raw[split_idx:]\n",
    "Y_train = Y_raw[:split_idx]\n",
    "Y_test = Y_raw[split_idx:]\n",
    "\n",
    "#print(X_train[0],'------',Y_train[0])\n",
    "#pint(split_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(input, input_length, output_dim=2, rnn_units=128, dropout=1):\n",
    "    input_ = tf.unstack(input, input_length, 1)\n",
    "    with tf.name_scope('RNN_Layer'):\n",
    "        with tf.variable_scope('lstm'):\n",
    "            lstm_cell = tf.nn.rnn_cell.LSTMCell(rnn_units, forget_bias=1.0)\n",
    "            outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, input_, dtype=tf.float32)\n",
    "        rnn_output = outputs[-1] #last output\n",
    "    with tf.name_scope('Layer'):\n",
    "        rnn_output = tf.nn.dropout(rnn_output,dropout)\n",
    "        W = tf.Variable(tf.random_normal([rnn_units,output_dim], 0.0, 0.1),name='W1')\n",
    "        b = tf.Variable(tf.random_normal([output_dim], 0.0, 0.1), name='Bias')\n",
    "        return tf.matmul(rnn_output,W)+b\n",
    "\n",
    "### Define the general input and output\n",
    "X  = tf.placeholder(\"float\", [None, input_length, input_dimension])\n",
    "Y  = tf.placeholder(\"float\", [None, 1, output_dimension])\n",
    "dr = tf.placeholder(\"float\") #dropout parameter\n",
    "\n",
    "prediction = RNN(X,input_length,output_dim=output_dimension,rnn_units=rnn_dim, dropout=dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Training_Stuff'):\n",
    "    loss = tf.reduce_mean(tf.square(prediction-Y))\n",
    "    optimizer   = tf.train.AdamOptimizer(0.00001)\n",
    "    train_op    = optimizer.minimize(loss)\n",
    "    train_sum   = tf.summary.scalar('Training_loss',loss)\n",
    "    test_sum    =  tf.summary.scalar('Validation_loss',loss)\n",
    "    file_writer = tf.summary.FileWriter('Tensorboard/len='+str(input_length)+'_rnn='+str(rnn_dim)+'/')\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.random.randint(0,len(X_train),len(X_train))\n",
    "batch_indexes = indexes[:batch_size]\n",
    "print(batch_indexes)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start training\n",
    "with tf.Session() as sess:\n",
    "    file_writer.add_graph(sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs*15):\n",
    "        #TODO: Create batches from original data\n",
    "        indexes = np.random.randint(0,len(X_train),len(X_train))\n",
    "        batch_n = 0\n",
    "        while batch_n*batch_size < len(X_train):\n",
    "            batch_indexes = indexes[batch_n*batch_size:(batch_n+1)*batch_size]\n",
    "            batch_n += 1\n",
    "            batch_x, batch_y = X_train[batch_indexes], Y_train[batch_indexes]\n",
    "            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, dr: 0.8})\n",
    "            \n",
    "        # Compute the current model loss over training and testing data\n",
    "        s,lr = sess.run([train_sum,loss],feed_dict={X:X_train,Y:Y_train,dr:1})\n",
    "        file_writer.add_summary(s,epoch)\n",
    "        s,le = sess.run([test_sum,loss],feed_dict={X:X_test,Y:Y_test,dr:1})\n",
    "        file_writer.add_summary(s,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
